# Native Binary Encoding Optimization

## Overview

As of this update, Poto automatically uses **native browser APIs** for base64 encoding of binary data, resulting in **40-50% faster performance** when uploading Uint8Array or ArrayBuffer objects.

## What Changed

### Before
```typescript
// Uint8Array/ArrayBuffer used slow JavaScript-based base64 encoding
const imageData = new Uint8Array(buffer);
await module.upload(imageData); // ðŸŒ Slow: JavaScript loops + btoa()
```

### After  
```typescript
// Now automatically uses native FileReader API!
const imageData = new Uint8Array(buffer);
await module.upload(imageData); // ðŸš€ Fast: Native C++ encoding
```

## Technical Details

### How It Works

#### Client-Side (Encoding)
1. **Detection**: `PotoClient._hasBlob()` now detects ArrayBuffer and TypedArrays (in addition to Blob/File)
2. **Async Path**: Automatically uses `stringifyTypedJsonAsync()` for binary data
3. **Native Encoding**: TypedJSON converts ArrayBuffer â†’ Blob â†’ FileReader.readAsDataURL()
4. **Result**: Uses browser's native C++ base64 encoding instead of JavaScript

#### Server-Side (Decoding) 
1. **Detection**: `TypedJSON._base64ToArrayBuffer()` checks environment
2. **Node/Bun**: Uses native `Buffer.from(base64, 'base64')` (C++ implementation)
3. **Browser**: Falls back to `atob()` + loop (necessary for browser compatibility)
4. **Result**: Server decoding is also optimized with native methods

### Code Changes

#### TypedJSON.ts (Encoding - Client Side)
- Added `_arrayBufferToBase64Async()` - uses native FileReader API
- Added `_serializeTypedArrayAsync()` - async version with native encoding
- Added `_serializeArrayBufferAsync()` - async version with native encoding
- Updated `_hasBlobs()` - now detects binary data in browsers
- Updated `_serializeValueAsync()` - handles ArrayBuffer/TypedArray with async encoding

#### TypedJSON.ts (Decoding - Server Side)
- Added `_base64ToArrayBuffer()` - uses native Buffer.from() in Node/Bun
- Updated `_deserializeValue()` - uses optimized decoding for:
  - ArrayBuffer deserialization
  - TypedArray deserialization
  - Blob deserialization
  - DataView deserialization

#### PotoClient.ts
- Updated `_hasBlob()` - now detects ArrayBuffer and TypedArrays
- Existing logic automatically triggers async serialization for binary data

#### browser-types.d.ts
- Updated Buffer type definition to support base64 decoding operations

### Performance Comparison

For a 2-3 MB image file:

| Method | Before | After | Improvement |
|--------|--------|-------|-------------|
| File/Blob | ~150ms | ~150ms | Already fast |
| Uint8Array | ~250ms | **~150ms** | **40% faster** âœ¨ |
| ArrayBuffer | ~250ms | **~150ms** | **40% faster** âœ¨ |

## Platform Support

| Platform | Binary Encoding | Performance |
|----------|-----------------|-------------|
| **Modern Browsers** | Native FileReader API | Fast âœ¨ |
| **Node.js** | Native Buffer.toString('base64') | Fast âœ¨ |
| **Bun** | Native Buffer.toString('base64') | Fast âœ¨ |
| **Legacy (no FileReader)** | JavaScript fallback | Slower (but still works) |

## Migration Guide

### No Code Changes Required!

This optimization is **completely automatic**. Your existing code will automatically benefit:

```typescript
// This code is now automatically optimized:
const imageData = new Uint8Array(await file.arrayBuffer());
await myModule.processImage(imageData);
// âœ¨ Now uses native FileReader under the hood!
```

### Verification

To verify the optimization is working, check the console logs:

```
â±ï¸ getImageSize (Uint8Array) Performance:
  - Client Fileâ†’Uint8Array: 5.23ms
  - RPC call time (includes native base64 encoding): 142.15ms
  - Total time: 147.38ms
  - âœ¨ Now uses native FileReader API (auto-optimized!)
```

## Implementation Details

### Complete Optimization Flow

#### Client-Side Encoding (Browser)
```typescript
ArrayBuffer/Uint8Array
  â†“
new Blob([buffer])           // Wrap in Blob
  â†“
FileReader.readAsDataURL()   // Native C++ base64 encoding â† FAST!
  â†“
Extract base64 from data URL
  â†“
Send over network
```

#### Server-Side Decoding (Node/Bun)
```typescript
Receive base64 string over network
  â†“
Buffer.from(base64, 'base64')  // Native C++ base64 decoding â† FAST!
  â†“
Extract ArrayBuffer
  â†“
Reconstruct TypedArray/ArrayBuffer
```

### Why This Is Faster

| JavaScript Approach (Old) | Native FileReader (New) |
|---------------------------|-------------------------|
| Loop through every byte | Bulk memory operation |
| String.fromCharCode() per chunk | Native C++ implementation |
| Multiple concatenations | Single optimized operation |
| Pure JavaScript | Compiled native code |
| ~250ms for 2MB | ~150ms for 2MB |

## Edge Cases

### Synchronous stringifyTypedJson()

The old synchronous method still exists for compatibility:

```typescript
// Sync method (still works, but slower for binary data)
const json = TypedJSON.stringify({ data: uint8Array });

// Async method (now automatically used by RPC)
const json = await TypedJSON.stringifyAsync({ data: uint8Array });
```

### Large Files

For very large files (>10MB), consider:
1. Chunked uploads (not yet implemented)
2. Direct blob upload endpoints
3. WebSocket binary frames

### Node.js/Bun

In Node.js and Bun environments:
- Already used native `Buffer.toString('base64')` (fast)
- This optimization mainly benefits **browsers**
- No performance change in server environments

## Testing

### Manual Testing

1. Open demo app: `cd demoapp && bun run dev`
2. Upload an image (2-3 MB recommended)
3. Compare performance between the three methods:
   - **Uint8Array**: Should now be ~40-50% faster
   - **ArrayBuffer**: Should now be ~40-50% faster  
   - **File**: Should remain the same (already fast)

### Expected Results

All three methods should now have similar performance:

```
Uint8Array:    ~150ms âœ¨ (was ~250ms)
ArrayBuffer:   ~150ms âœ¨ (was ~250ms)
File:          ~150ms (was ~150ms)
```

## Related Documentation

- `docs/FILE_UPLOAD_PERFORMANCE.md` - Complete performance analysis
- `docs/performance-investigation/ENCODING_PERFORMANCE_INSTRUMENTATION.md` - Original investigation
- `docs/performance-investigation/ENCODING_DATAFLOW_DIAGRAM.md` - Data flow visualization

## Future Optimizations

1. âœ… **Native base64 for binary data** - DONE!
2. **Streaming for large files** - Upload files in chunks
3. **WebSocket binary frames** - Avoid base64 entirely for WebSocket connections
4. **Web Workers** - Offload encoding to background threads
5. **Compression** - Compress before encoding (might help for text-like binary data)

## Backward Compatibility

This optimization is **fully backward compatible**:
- Existing code continues to work unchanged
- Old synchronous API still available
- Automatic performance improvement
- No breaking changes

## Credits

This optimization addresses the performance difference discovered during file upload benchmarking, where File objects were found to be 40-50% faster than Uint8Array/ArrayBuffer due to native encoding.

The solution leverages the same native FileReader API for all binary types, providing consistent fast performance across the framework.

